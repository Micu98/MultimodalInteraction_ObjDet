{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Identify Object Positions in Images - YOLO vs VLM "]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["\n", "image 1/1 /Users/gava/Documents/Teaching/Master/DataScience/MultimodalInteraction_ObjDet/images/table_scene.jpeg: 640x640 1 cup, 2 potted plants, 2 dining tables, 4 books, 3 vases, 92.1ms\n", "Speed: 3.6ms preprocess, 92.1ms inference, 10.7ms postprocess per image at shape (1, 3, 640, 640)\n", "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n", "Bounding boxes for 'cup': [[615.28662109375, 553.3635864257812, 845.9586181640625, 780.4920654296875]]\n"]}], "source": ["# Import YOLO and load a pre-trained model\n", "from ultralytics import YOLO\n", "\n", "# Load the YOLOv8 pre-trained model\n", "model = YOLO('yolov8n.pt')  # nano model for quick inference\n", "\n", "# Get the class ID for the target object\n", "target_object_name = \"cup\"  # Replace with your target object name\n", "class_names = model.names\n", "target_class_id = next((class_id for class_id, name in class_names.items() if name == target_object_name), None)\n", "\n", "if target_class_id is not None:\n", "    # Perform inference\n", "    results = model('images/table_scene.jpeg', save = True)  # Replace with your image path\n", "\n", "    # Filter bounding boxes for the target object\n", "    detections = results[0].boxes\n", "    specific_boxes = [\n", "        box.xyxy[0].tolist()\n", "        for box in detections\n", "        if int(box.cls[0]) == target_class_id\n", "    ]\n", "\n", "    print(f\"Bounding boxes for '{target_object_name}': {specific_boxes}\")\n", "else:\n", "    print(f\"Object name '{target_object_name}' not found in the model's class names.\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Utils "]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# @title Plotting Utils\n", "import json\n", "import random\n", "import io\n", "from PIL import Image, ImageDraw\n", "from PIL import ImageColor\n", "\n", "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n", "\n", "def plot_bounding_boxes(im, noun_phrases_and_positions):\n", "    \"\"\"\n", "    Plots bounding boxes on an image with markers for each noun phrase, using PIL, normalized coordinates, and different colors.\n", "\n", "    Args:\n", "        img_path: The path to the image file.\n", "        noun_phrases_and_positions: A list of tuples containing the noun phrases\n", "         and their positions in normalized [y1 x1 y2 x2] format.\n", "    \"\"\"\n", "\n", "    # Load the image\n", "    img = im\n", "    width, height = img.size\n", "    print(img.size)\n", "    # Create a drawing object\n", "    draw = ImageDraw.Draw(img)\n", "\n", "    # Define a list of colors\n", "    colors = [\n", "    'red',\n", "    'green',\n", "    'blue',\n", "    'yellow',\n", "    'orange',\n", "    'pink',\n", "    'purple',\n", "    'brown',\n", "    'gray',\n", "    'beige',\n", "    'turquoise',\n", "    'cyan',\n", "    'magenta',\n", "    'lime',\n", "    'navy',\n", "    'maroon',\n", "    'teal',\n", "    'olive',\n", "    'coral',\n", "    'lavender',\n", "    'violet',\n", "    'gold',\n", "    'silver',\n", "    ] + additional_colors\n", "\n", "    # Iterate over the noun phrases and their positions\n", "    for i, (noun_phrase, (y1, x1, y2, x2)) in enumerate(\n", "        noun_phrases_and_positions):\n", "        # Select a color from the list\n", "        color = colors[i % len(colors)]\n", "\n", "        # Convert normalized coordinates to absolute coordinates\n", "        abs_x1 = int(x1/1000 * width)\n", "        abs_y1 = int(y1/1000 * height)\n", "        abs_x2 = int(x2/1000 * width)\n", "        abs_y2 = int(y2/1000 * height)\n", "\n", "        # Draw the bounding box\n", "        draw.rectangle(\n", "            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4\n", "        )\n", "\n", "        # Draw the text\n", "        draw.text((abs_x1 + 8, abs_y1 + 6), noun_phrase, fill=color)\n", "\n", "    # Display the image\n", "    img.show()\n", "\n", "# @title Parsing utils\n", "def parse_list_boxes(text):\n", "  result = []\n", "  for line in text.strip().splitlines():\n", "    # Extract the numbers from the line, remove brackets and split by comma\n", "    try:\n", "      numbers = line.split('[')[1].split(']')[0].split(',')\n", "    except:\n", "      numbers =  line.split('- ')[1].split(',')\n", "\n", "    # Convert the numbers to integers and append to the result\n", "    result.append([int(num.strip()) for num in numbers])\n", "\n", "  return result\n", "\n", "def parse_list_boxes_with_label(text):\n", "  text = text.split(\"```\\n\")[0]\n", "  return json.loads(text.strip(\"```\").strip(\"python\").strip(\"json\").replace(\"'\", '\"').replace('\\n', '').replace(',}', '}'))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# VLM (1): OPEN-AI"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["from openai import OpenAI\n", "import os\n", "import base64\n", "\n", "# Function to encode the image\n", "def encode_image(image_path):\n", "  with open(image_path, \"rb\") as image_file:\n", "    return base64.b64encode(image_file.read()).decode('utf-8')\n", "\n", "openAIclient = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"))\n", "img = \"images/table_scene.jpeg\"\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["#basic call to gpt4 with prompt and image\n", "\n", "completion = openAIclient.chat.completions.create(\n", "    model=\"gpt-4o-mini\",\n", "    messages=[\n", "        {\n", "            \"role\": \"user\",\n", "            \"content\": [\n", "                {\"type\": \"text\", \"text\": \"Detect if there is a cup in the image and reutrn its coordinates as a list in the format [ymin,xmin, ymax, xmax]. Just output the list.\"},\n", "                {\n", "                    \"type\": \"image_url\",\n", "                    \"image_url\": {\n", "                        \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n", "                    }\n", "                },\n", "            ],\n", "        }\n", "    ],\n", ")\n", "\n", "# Wrap the text to a specified width\n", "\n", "response = str(completion.choices[0].message.content)\n"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": ["'[411, 598, 469, 635]'"]}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": ["response"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(1024, 1024)\n"]}], "source": ["boxes= parse_list_boxes(response)\n", "boxes = {f'cup_{i}': x for i, x in enumerate(boxes)}\n", "plot_bounding_boxes(Image.open(img), noun_phrases_and_positions=list(boxes.items()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# VLM (2): GEMINI"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["/Users/gava/.venv/openai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n", "  from .autonotebook import tqdm as notebook_tqdm\n"]}], "source": ["%matplotlib inline\n", "import os\n", "import google.generativeai as genai\n", "from PIL import Image\n", "import requests\n", "import io"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["[543, 597, 756, 821]\n"]}], "source": ["im = Image.open(img)\n", "\n", "genai.configure(api_key=os.environ.get(\"GEMINI_API_KEY\"))\n", "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n", "\n", "response = model.generate_content([\n", "    im,\n", "    (\n", "        \"Detect if there is a cup in the image and reutrn its coordinates as a list in the format '[ymin,xmin, ymax, xmax]'. Just output the list.\\n \"\n", "    ),\n", "])\n", "response.resolve()\n", "print(response.text)"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(1024, 1024)\n"]}], "source": ["boxes= parse_list_boxes(response.text)\n", "boxes = {f'cup_{i}': x for i, x in enumerate(boxes)}\n", "plot_bounding_boxes(im, noun_phrases_and_positions=list(boxes.items()))"]}], "metadata": {"kernelspec": {"display_name": "Python (openai)", "language": "python", "name": "openai_env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}